---
title: "Data cleaning"
author: "Katrina Keegan"
date: "10/13/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidygraph)
library(ggraph)
library(igraph)
library(readxl)
library(tm)
library(urltools)
library(corpus)
library(splus2R)
library(SnowballC)
library(keyATM)
library(rstanarm)
library(gt)
library(gtsummary)
library(broom.mixed)
```


```{r}
#Do data cleaning in this file 
#To input data do here saveRDS(tibble, file = tibble.RDS)
#Then readRDS in the server file
```

```{r network}
#The following section is dedicated to creating the NETWORK
  
  #Below is a function that will clean the data so that I have the relevant network data
#It takes data scraped from telegram and converted to csv through https://json-csv.com/
#And outputs just the columns with network info (links and forwarded from), with NAs in both removed

  clean_to_network_cols <- function(data) {
    data %>%
      select(messages__text__href, messages__forwarded_from) %>%
      rename(link = messages__text__href, forwarded = messages__forwarded_from) %>%
      filter(!is.na(link) | !is.na(forwarded)) 
  }
  
  #Here is a function that combines information from the links and shares by changing link names to group names
  #Change telegram links into forwarded format
  
  change_link_to_name <- function(data) {
    data %>%
      mutate(network =
               case_when(!is.na(forwarded) ~ forwarded,
                         grepl("BlackBookBelarus", link) ~"Черная книга Беларуси",
                         grepl("joinchat/FOFVqhyzx5Gd3B", link) ~"Neighborhood group",
                         grepl("luxta_tv", link) ~"LUXTA",
                         grepl("minsk_eastern_district_107_chat", link) ~"Neighborhood Example 1",
                         grepl("nexta_live", link) ~"NEXTA Live",
                         grepl("nexta_tv", link) ~"NEXTA",
                         grepl("bel_girls", link) ~"GIRL POWER BELARUS",
                         grepl("BelarusTelegram", link) ~"Беларуский Телеграм",
                         grepl("Belatp", link) ~"Типичная Беларусь",
                         grepl("bysol", link) ~"Фонд солидарности BYSOL",
                         grepl("cpartisans", link) ~"Кибер-Партизаны",
                         grepl("euroradio", link) ~"Euroradio",
                         grepl("grankovski", link) ~"Neighborhood group",
                         grepl("honestpeople_by", link) ~"Честные люди",
                         grepl("joinchat/AAAAAEjffyoi9hgp9kVhTA", link) ~"МАЯ КРАІНА БЕЛАРУСЬ",
                         grepl("joinchat/AAAAAEklfX7sPNSO8QXhJw", link) ~"Усы Лукашенко",
                         grepl("joinchat/AAAAAFPezXkJRKSNTyAm7w", link) ~"Типичная Беларусь",
                         grepl("joinchat/H6JljhkJIaEk321KqY6YNA", link) ~"Neighborhood group",
                         grepl("kraina97", link) ~"97%",
                         grepl("kupalovskiy95", link) ~"Neighborhood group",
                         grepl("lifeyt", link) ~"ЖЮ",
                         grepl("MKBelbot", link) ~"МАЯ КРАІНА БЕЛАРУСЬ",
                         grepl("motolkohelp", link) ~"#МотолькоПомоги этому городишко от 3% избавиться",
                         grepl("nashaniva", link) ~"Наша Нiва",
                         grepl("okrug99minsk", link) ~"Neighborhood group",
                         grepl("otzyv_lagunova", link) ~"Neighborhood group",
                         grepl("pulpervoi", link) ~"Светлана Тихановская",
                         grepl("recall_deputy_lenchevskaya", link) ~"Neighborhood Example 2",
                         grepl("recall_deputy_starovoitova", link) ~"Neighborhood group",
                         grepl("rh_by", link) ~"Рэгіянальная газета",
                         grepl("rian_ru", link) ~"РИА Новости",
                         grepl("stachkom", link) ~"Стачком ОАО Беларуськалий",
                         grepl("stop_dick", link) ~"Stop Dick",
                         grepl("stop_dumbadze", link) ~"Neighborhood group",
                         grepl("thisminsk", link) ~"Это Минск, детка!",
                         grepl("tutby_official", link) ~"TUT.BY новости",
                         grepl("zhodino2020", link) ~"Координация помощи в Жодино",
                         grepl("ATN_BTRC", link) ~"ATN_NEWS",
                         grepl("bajby", link) ~"БАЖ / BAJ",
                         grepl("dimsmirnov175", link) ~"Пул N3",
                         grepl("minzdravbelarus", link) ~"Официальный Минздрав",
                         grepl("nvonlineby", link) ~"Народная Воля (газета-сайт)",
                         grepl("radiosvaboda", link) ~"Радыё Свабода — Беларусь",
                         grepl("rbc_news", link) ~"РБК",
                         grepl("robabayan", link) ~"Роман Бабаян",
                         grepl("studenty2020", link) ~"Студэнцкая ініцыятыўная група",
                         grepl("tass_agency", link) ~"ТАСС",
                         grepl("viktarbabarykaofficial", link) ~"Виктор Бабарико — официальный канал",
                         grepl("vladimirlegoyda", link) ~"Владимир Легойда",
                         grepl("stop_gaiduk", link) ~"Neighborhood Example 3",
                         grepl("lukanomika", link) ~"ЭКАНОМІКА",
                         grepl("belsat", link) ~"Белсат",
                         grepl("bazabazon", link) ~"Baza",
                         grepl("belarus_blacklist_bot", link) ~"Черная книга Беларуси",
                         grepl("durov", link) ~"Durov's Channel",
                         grepl("tsikhanouskaya", link) ~"Светлана Тихановская",
                         grepl("jivaiagazeta", link) ~"Пенсионеры 97%. Инфо",
                         grepl("narod97", link) ~"Чат 97%",
                         grepl("pul_1", link) ~"Пул Первого",
                         grepl("belamova", link) ~"Беларусь головного мозга",
                         grepl("bnkbel", link) ~"Баста!",
                         grepl("gomeltoday", link) ~"Сильные Новости - gomel.today",
                         grepl("UsyLukashenko", link) ~"Усы Лукашенко",
                         grepl("RealnaiaBelarus", link) ~"Реальная Беларусь",
                         grepl("joinchat/AAAAAEFpAvewJ_3iThy5lA", link) ~"Горячие Новости",
                         grepl("sadmika", link) ~"Грустный Коленька",
                         grepl("MikolaDziadok", link) ~"MIKOLA",
                         grepl("worldprotest", link) ~"Протесты в мире",
                         grepl("vybary", link) ~"Выборы. Беларусь",
                         grepl("belarus_economy", link) ~"Экономика Беларуси",
                         grepl("belarusian_history", link) ~"Belarus history",
                         grepl("belarus_lite", link) ~"Пятая колонка",
                         grepl("tginfo", link) ~"Telegram Info",
                         grepl("mkbelarus", link) ~"МАЯ КРАІНА БЕЛАРУСЬ",
                         grepl("listovki_97", link) ~"Листовки 97%",
                         grepl("palchys", link) ~"PALCHYS",
                         grepl("latushka", link) ~"Павел Латушко | Pavel Latushka",
                         grepl("iSANS_Belarus", link) ~"iSANS Беларусь",
                         grepl("BlackBookBrest", link) ~"Черная книга local",
                         grepl("BlackBookVitebsk", link) ~"Черная книга local",
                         grepl("BlackBookGomel", link) ~"Черная книга local",
                         grepl("BlackBookGrodno", link) ~"Черная книга local",
                         grepl("BlackBookMogilev", link) ~"Черная книга local",
                         grepl("CyberPartisan", link) ~"Кибер Партизаны",
                         grepl("bychangenews", link) ~"ByChange",
                         grepl("onlinerby", link) ~"Onliner",
                         grepl("naukaforpeople", link) ~"Забавная Наука",
                         grepl("prirodaforpeople", link) ~"Жэсточайшая Природа",
                         grepl("twarforpeople", link) ~"Жэсточайшие Твари",
                         grepl("cyberpartisan", link) ~"Кибер Партизаны",
                         grepl("pramenby", link) ~"Прамень",
                         grepl("elehtarat", link) ~"Элехтарат",
                         grepl("joinchat/Iv257x0g4SFPQBYeW-pdow", link) ~"Neighborhood group",
                         grepl("joinchat/AAAAAEklfX5GcSBexFNnKA", link) ~"Усы Лукашенко",
                         grepl("joinchat/AAAAAEklfX5GcSBexFNnKA", link) ~"Усы Лукашенко",
                         grepl("viasna96", link) ~"Вясна / Правы чалавека ў Беларусі",
                         grepl("studentyBY", link) ~"отчислено",
                         grepl("s_bdtu", link) ~"Салідарны БДТУ",
                         grepl("rada_vision", link) ~"Координационный совет",
                         grepl("pressmvd", link) ~"Пресс-секретарь МВД Беларуси",
                         grepl("pressballby", link) ~"Спорт Беларуси — Прессбол",
                         grepl("poezdBY", link) ~"Молодежный поезд",
                         grepl("peaceful_bsuir", link) ~"Мирный БГУИР",
                         grepl("minskoemetro", link) ~"Минский метрополитен",
                         grepl("kyky_org", link) ~"Кулуары KYKY",
                         grepl("krumkachy", link) ~"Krumkachy",
                         grepl("klaskouski", link) ~"Klaskouski",
                         grepl("intexpress", link) ~"Интекс-Пресс Барановичи",
                         grepl("golosby_bot", link) ~"Голос",
                         grepl("eu_by", link) ~"Европейская Беларусь",
                         grepl("cvboard", link) ~"CV board: Honest people/HR Breakfast",
                         grepl("binklbinkl", link) ~"БИНОКЛЬ",
                         grepl("belxalat", link) ~"БЕЛЫЕ ХАЛАТЫ",
                         grepl("belteanews", link) ~"Чай з малинавым варэннем",
                         grepl("belarusseichas", link) ~"Беларусь Сейчас",
                         grepl("belarusin", link) ~"ЧП Беларусь",
                         grepl("mbkhmedia", link) ~"МБХ медиа",
                         grepl("khliabets", link) ~"Khliabets",
                         grepl("MSTBELARUS", link) ~"Пресс-служба Министерства спорта и туризма",
                         grepl("LptSM1dPAC3ro5_GRi8E1Q", link) ~"Каментатарская",
                         grepl("hrydzinphoto", link) ~"Hrydzin Phototography",
                         grepl("novaya_pishet", link) ~"Новая газета",
                         grepl("aavst55", link) ~"aavst",
                         grepl("UnileverRussiaNews", link) ~"Unilever в России: о бизнесе, брендах и людях",
                         grepl("menskdoldrums", link) ~"Менская хандра",
                         grepl("tabakerka_by", link) ~"Беларусь бастует!",
                         grepl("polotsk_novopolotsk", link) ~"Полоцк - Новополоцк Беларусь",
                         grepl("rt_russian", link) ~"RT на русском",
                         grepl("terroristybelarusi", link) ~"Каратели Беларуси - имена, адреса, родственники",
                         grepl("praludzej", link) ~"Пра людзей. Беларусь",
                         grepl("progomel", link) ~"Neighborhood group",
                         grepl("mediazona_by", link) ~"Медиазона. Беларусь",
                         grepl("molovTG", link) ~"Молодечно HOME",
                         grepl("the_village_me", link) ~"The Village Беларусь",
                         grepl("vybory_smotri", link) ~"Выборы видишь? | TUT.BY Политика",
                         grepl("spadczyna", link) ~"Спадчына",
                         grepl("skgovby", link) ~"Следственный комитет Беларуси",
                         grepl("horadnia", link) ~"Haradzenski",
                         grepl("hrodnalife", link) ~"Hrodna.life",
                         grepl("vitebsk97pro", link) ~"Витебск 97%",
                         grepl("polotsk_i_novopolotsk", link) ~"Полоцк и Новополоцк",
                         grepl("ntvnews", link) ~"НТВ",
                         grepl("lebiadok", link) ~"Lebiadok",
                         grepl("diarius_corporatum", link) ~"Дыярыуш карпаранцкі",
                         grepl("breakingmash", link) ~"Mash",
                         grepl("minsk_new", link) ~"Минск: какой народец - такой и городишко",
                         grepl("iznanka_news", link) ~"Изнанка 18+ | Украина",
                         grepl("spiski_okrestina", link) ~"Списки Задержанных",
                         grepl("real_kyiv", link) ~"Real Kyiv",
                         grepl("meduzalive", link) ~"Медуза — LIVE",
                         grepl("codaru", link) ~"CodaRU",
                         grepl("belarus_mythology", link) ~"Мифология Беларуси",
                         grepl("protest_by", link) ~"Честные новости",
                         grepl("koko_by", link) ~"Мерзкий Кокобай", 
                         grepl("brestgazetatelegram", link) ~"Брестская газета: новости - 2020",
                         grepl("BelGram", link) ~ "Беларусь Телеграм Чат",
                         grepl("suh97pro", link) ~"Neighborhood group",
                         grepl("MayakMinsk", link) ~"Neighborhood group", 
                         grepl("sovbelorussia", link) ~"Советская Белоруссия",
                         grepl("naviny_by", link) ~"Naviny.by",
                         grepl("joinchat/BRXqnBthP4Ow7jxmEQK8Dw", link) ~ "Neighborhood group",
                         grepl("joinchat/FVRJK1JydYL_NcAjzr1eXQ", link) ~ "Neighborhood group",
                         grepl("Len95okrug", link) ~ "Neighborhood group",
                         grepl("brestcomment", link) ~"Брест - обсуждаем новости города", 
                         grepl("grodno015by", link) ~ "Сайт Гродно 015.BY", 
                         grepl("gazetaby", link) ~"Салідарнасць – главные новости Беларуси",
                         grepl("tribuna_by", link) ~"Tribuna.com Беларусь", 
                         grepl("Bell_tech", link) ~"The Bell Tech",
                         grepl("gpkgovby", link) ~"Пограничный комитет Беларуси",
                         grepl("TheCitizenBelarus", link) ~"Гражданин",
                         grepl("telegrambelarus", link) ~"Telegram Беларусь",
                         grepl("harbatsevich", link) ~"HARBACEVIČ",
                         grepl("zaslavl_new", link) ~"Заславский чат активистов",
                         grepl("vottaktv", link) ~"Vot Tak TV",
                         grepl("CatalogTelegram", link) ~"Каталог Telegram", 
                         grepl("bnr100by", link) ~"Каманда БНР100",
                         grepl("symbalby", link) ~"SYMBAL.BY",
                         grepl("joinchat/AAAAAEwwDXiIvnT0OwM8xQ", link) ~"Будни Беларуси",
                         grepl("krasnyj_bor", link) ~"Neighborhood group"
               )
      )
  }
  
  #The following function will count the total number of connections (both links and shares)
  count_connections <- function(data) {
    data %>%
      group_by(network) %>%
      summarize(count = n(), .groups = "drop") %>%
      arrange(desc(count))
  }
  
  #This function combines the above functions to read my data and clean it
  create_connections <- function(data, name) {
    read_csv(data, col_types = cols(
      .default = col_character(),
      id = col_double(),
      messages__id = col_double(),
      messages__date = col_datetime(format = ""),
      messages__edited = col_datetime(format = ""),
      messages__from_id = col_double(),
      messages__duration_seconds = col_double(),
      messages__width = col_double(),
      messages__height = col_double(),
      messages__reply_to_message_id = col_double()
    )) %>%
      clean_to_network_cols() %>%
      change_link_to_name() %>%
      count_connections() %>%
      mutate(source = name) %>%
      filter(source != network) %>%
      filter(!is.na(network))
  }
  
  #Now I actually enter all my data and clean it
  nexta_live_connections <- create_connections("raw_data_csv/nexta_live.csv", "NEXTA Live")
  tutby_connections <- create_connections("raw_data_csv/tutby_official_raw.csv", "TUT.BY новости")
  nexta_tv_connections <- create_connections("raw_data_csv/raw_nexta_tv.csv", "NEXTA")
  belamova_connections <- create_connections("raw_data_csv/belamova.csv", "Беларусь головного мозга")
  mkbelarus_connections <- create_connections("raw_data_csv/mkbelarus.csv", "МАЯ КРАІНА БЕЛАРУСЬ")
  nashaniva_connections <- create_connections("raw_data_csv/nashaniva.csv", "Наша Нiва")
  onliner_connections <- create_connections("raw_data_csv/onliner.csv", "Onliner")
  euroradio_connections <- create_connections("raw_data_csv/euroradio.csv", "Euroradio")
  motolkohelp_connections <- create_connections("raw_data_csv/motolkohelp.csv", "#МотолькоПомоги этому городишко от 3% избавиться")
  kyky_connections <- create_connections("raw_data_csv/kyky.csv", "Кулуары KYKY")
  belsat_connections <- create_connections("raw_data_csv/belsat.csv", "Белсат")
  radiosvaboda_connections <- create_connections("raw_data_csv/radiosvaboda.csv", "Радыё Свабода — Беларусь")
  usylukashenko_connections <- create_connections("raw_data_csv/usylukashenko.csv", "Усы Лукашенко")
  tsikhanouskaya_connections <- create_connections("raw_data_csv/tsikhanouskaya.csv", "Светлана Тихановская")
  blackbookbelarus_connections <- create_connections("raw_data_csv/blackbookbelarus.csv", "Черная книга Беларуси")
  belteanews_connections <- create_connections("raw_data_csv/belteanews.csv", "Чай з малинавым варэннем")
  tribuna_by_connections <- create_connections("raw_data_csv/tribuna_by.csv", "Tribuna.com Беларусь")
  studenty_by_connections <- create_connections("raw_data_csv/studenty_by.csv", "отчислено")
  pressmvd_connections <- create_connections("raw_data_csv/pressmvd.csv", "Пресс-секретарь МВД Беларуси")
  mc_maxim_connections <- create_connections("raw_data_csv/mc_maxim.csv", "Работаем с Пушкиным!")
  viasna96_connections <- create_connections("raw_data_csv/viasna96.csv", "Вясна / Правы чалавека ў Беларусі")
  bajby_connections <- create_connections("raw_data_csv/bajby.csv", "БАЖ / BAJ")
  minsk_eastern_district_107_chat_connections <- create_connections("raw_data_csv/minsk_eastern_district_107_chat.csv", "Neighborhood Example 1")
  recall_deputy_lenchevskaya_connections <- create_connections("raw_data_csv/recall_deputy_lenchevskaya.csv", "Neighborhood Example 2")
  stop_gaiduk_connections <- create_connections("raw_data_csv/stop_gaiduk.csv", "Neighborhood Example 3")
  vybory_smotri_connections <- create_connections("raw_data_csv/vybory_smotri.csv", "Выборы видишь? | TUT.BY Политика")
  
  #Now I combine all of those into the whole network
  big_network <- bind_rows(nexta_live_connections, 
                           tutby_connections,
                           nexta_tv_connections,
                           belamova_connections,
                           mkbelarus_connections,
                           nashaniva_connections,
                           onliner_connections,
                           euroradio_connections,
                           motolkohelp_connections,
                           kyky_connections,
                           belsat_connections,
                           radiosvaboda_connections,
                           usylukashenko_connections,
                           tsikhanouskaya_connections,
                           blackbookbelarus_connections,
                           belteanews_connections,
                           tribuna_by_connections,
                           studenty_by_connections,
                           pressmvd_connections,
                           mc_maxim_connections,
                           viasna96_connections,
                           bajby_connections,
                           minsk_eastern_district_107_chat_connections,
                           recall_deputy_lenchevskaya_connections,
                           stop_gaiduk_connections,
                           vybory_smotri_connections) %>%
    relocate(source, .before = network)

saveRDS(big_network, file = "clean_data/big_network.RDS")


```


```{r additions to big_network}
#Here I want to create a tibble with the following info for the corpus:
#Number of followers 


#counts the total number of times it was referenced by others 
links_to_channel <- big_network %>%
  group_by(network) %>%
  summarize(total_links_to_channel = sum(count), .groups = "drop") %>%
  filter(!is.na(network)) %>%
  rename(channel = network)

#Counts the total times it references others 
links_from_channel <- big_network %>%
  group_by(source) %>%
  summarize(total_links_from_channel = sum(count), .groups = "drop") %>%
  filter(!is.na(source)) %>%
  rename(channel = source)

#Now combine them and create the difference column
total_links <- links_from_channel %>%
  left_join(links_to_channel, by = "channel") %>%
  mutate(total_links_diff = total_links_from_channel - total_links_to_channel) %>%
  mutate(source_hub_by_total_links = case_when(
    total_links_diff > 0 ~ "hub",
    TRUE ~ "source"
  ))

#Counting the number of unique channels that link to it 
unique_links_to_channel <- big_network %>%
  group_by(network) %>%
  summarize(unique_links_to_channel = n(), .groups = "drop") %>%
  rename(channel = network)

#Counting the number of unique channels that it links to
unique_links_from_channel <- big_network %>%
  group_by(source) %>%
  summarize(unique_links_from_channel = n(), .groups = "drop") %>%
  rename(channel = source)

#Now combine them and create the difference column
unique_links <- unique_links_from_channel %>%
  left_join(unique_links_to_channel, by = "channel") %>%
  mutate(unique_links_diff = unique_links_from_channel - unique_links_to_channel) %>%
  mutate(source_hub_by_unique_links = case_when(
    unique_links_diff > 0 ~ "hub",
    TRUE ~ "source"
  ))

telegram_corpus <- read_excel("telegram_corpus.xlsx") %>%
  rename(channel = Name) %>%
  rename(subscribers = `Subscribers_10/15/2020`) %>%
  select(channel, subscribers, url, Type, Sources, language)

corpus_network_info <- full_join(total_links, unique_links, by = "channel") %>%
  mutate(source_hub = case_when(
  grepl("hub", source_hub_by_unique_links) & grepl("hub", source_hub_by_total_links) ~ "hub",
  grepl("source", source_hub_by_unique_links) & grepl("source", source_hub_by_total_links) ~ "source",
  TRUE ~ "unclear"
  )) %>%
  inner_join(telegram_corpus, by = "channel")

saveRDS(corpus_network_info, "gov50-final-shiny/clean_data/corpus_network_info.RDS")
  
```

```{r model about subscribers}
model1 <- stan_glm(data = corpus_network_info,
         formula = subscribers ~ total_links_from_channel + total_links_to_channel + unique_links_from_channel + unique_links_to_channel,
         refresh = 0
         )

tbl_regression(model1, intercept = TRUE) %>%
  as_gt() %>%
    tab_header(title = "Regression of links on subscribers") 
  
  

```



```{r plot for later 1}
#Plots to copy and paste later
corpus_network_info %>%
  ggplot(aes(x = subscribers, 
             y = fct_reorder(url, subscribers),
             fill = source_hub)) +
    geom_col() +
    labs(title = "Subscribers by channel",
         x = "Number of subscribers",
         y = "Channel URL",
         fill = " ") +
    theme_bw() +
    theme(legend.position = "top", legend.text = element_text(size = 7)) +
    scale_fill_discrete(labels = c("Hub (more links to other channels)", "Source (more links from other channels)", "Unclear"))
```

```{r plot for later 2}
corpus_network_info %>%
  pivot_longer(cols = c(total_links_from_channel, total_links_to_channel), names_to = "from_to_total") %>%
  ggplot(aes(x = subscribers, y = value, color = from_to)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = lm,
              formula = y ~ x,
              se = FALSE) +
  theme_bw() +
  labs(title = "Relationship between number of subscribers and channel links",
       color = " ",
       y = " ",
       x = "subscribers") +
  theme(legend.position = "bottom") + 
  scale_color_discrete(labels = c("Links and shares from channel", "Links and shares to channel"))
  

```



```{r messing around}

big_network_greater20 <- big_network %>%
  filter(count > 9)

most_connected <- big_network %>%
  group_by(network) %>%
  summarize(total_links = sum(count), .groups = "drop") %>%
  arrange(desc(total_links)) %>%
  slice(1:21) %>%
  filter(!is.na(network))

most_connected %>%
  ggplot(aes(x = total_links, y = fct_reorder(network, total_links))) +
    geom_col()
```


```{r small network graph}
#this section is for making the network graph

#I followed instructions here: https://www.jessesadler.com/post/network-analysis-with-r/
#https://www.youtube.com/watch?v=0xsM0MbRPGE

#Here I am creating a smaller network with just the connections that relate to the 26 for which I gathered data 

#This little trick of network %in% source worked beautifully to take only those observations for which the network data is also one of the sources

small_network <- big_network %>%
  filter(network %in% source)

#This just creates a list of the nodes and gives them an id number

small_nodes <- small_network %>%
  distinct(network) %>%
  rename(label = network) %>%
  rowid_to_column("id") %>%

#Now adding other info form corpus_network_info
  
  full_join(corpus_network_info, by = c("label" = "channel")) 

#This creates a list of edges using the id numbers
#The first step is to do a left join that adds the id numbers of the network to the matching id , and then rename that as "to"
#Then I do the same except for the sources, renaming the label to "from"
#Actually for some reason had to be reversed which is making my head hurt to logically think through but the code below does work correctly 

saveRDS(small_nodes, "gov50-final-shiny/clean_data/small_nodes.RDS")

small_edges <- small_network %>%
  left_join(small_nodes, by = c("source" = "label")) %>%
  rename(to = id) %>%
  left_join(small_nodes, by = c("network" = "label")) %>%
  rename(from = id) %>%
  rename(weight = count) %>%
  select(to, from, weight)

saveRDS(small_edges, "gov50-final-shiny/clean_data/small_edges.RDS")

#D is the edges, and vertices argument is for the nodes.

small_network_graph <- graph_from_data_frame(d = small_edges, directed = T, vertices = small_nodes)

#V(network_graph)$degree <- degree(network_graph)

#no igraph plot.igraph(small_network_graph,
            #vertex.label.cex = 0.6, #reduces font size
            #edge.arrow.size = 0.3, #Need edge here not just arrow.size
            #vertex.size = 10,
            #edge.width = E(small_network_graph)$weight / 100,
            #edge.curved = 0.3
            #) 

# Not using igraph plot.igraph(network_graph, 
           # vertex.color = "white",
           # vertex.label = NA,
           # edge.arrow.size = network_graph$count,
            #vertex.size = V(network_graph)$degree * 0.5)


#Plot 1: simple graph with arrows and labels 

ggraph(small_network_graph, layout = "lgl") +
  geom_node_text(aes(label = V(small_network_graph)$url),
                 size = 2,
                 color = "red") +
  
  #Arrow code copied from https://www.data-imaginist.com/2017/ggraph-introduction-edges/ 
  
  geom_edge_link(aes(alpha = sqrt(weight)),
                 arrow = arrow(length = unit(4, 'mm')),
                 end_cap = circle(3, 'mm'))

#Hover functionality https://stackoverflow.com/questions/38917101/how-do-i-show-the-y-value-on-tooltip-while-hover-in-ggplot2 

#Plot 2: graph without arrows, with source/hub info and subscribers
ggraph(small_network_graph, layout = "lgl") +
  geom_edge_fan(aes(alpha = sqrt(weight))) +
  geom_node_point(aes(color = V(small_network_graph)$source_hub),
                  size = sqrt(V(small_network_graph)$subscribers/10000)) +
  scale_color_manual(values = c("lightblue", "mediumpurple2", "lightpink")) +
  theme_void() +
  geom_node_text(aes(label = V(small_network_graph)$url),
                 size = 2,
                 color = "darkred")


# adding network statistics that could be useful in plotting

V(small_network_graph)$indegree <- degree(small_network_graph, mode = "in")

V(small_network_graph)$outdegree <- degree(small_network_graph, mode = "out")

#I betweenness is a directed or undirected measure but no "in" or "out" distinction, alos I added the weights from the edges. 

#Needed to add as_numeric so it would be treated as continuous. 

V(small_network_graph)$betweenness <- as.numeric( betweenness(small_network_graph, directed = TRUE, weights = E(small_network_graph)$weight))

#Graph 3: betweenness and indegree

#Just ask Tyler
ggraph(small_network_graph, layout = "lgl") +
  geom_edge_fan(aes(alpha = sqrt(weight))) +
  geom_node_point(aes(size = V(small_network_graph)$indegree,
                      color = V(small_network_graph)$betweenness)) +
  scale_color_gradient(low = "blue", high = "red") +
  theme_void() +
  geom_node_text(aes(label = V(small_network_graph)$url),
                 size = 2,
                 color = "darkred")

```

```{r big network graph}

#This section is about creating the big network graph

big_nodes <- big_network %>%
  distinct(network) %>%
  rename(label = network) %>%
  rowid_to_column("id") %>%
  
  #Adding a logical for in corpus or not
  
  mutate(in_corpus = ifelse(label %in% small_nodes$label, TRUE, FALSE)) %>%
  left_join(corpus_network_info, by = c("label" = "channel")) 

saveRDS(big_nodes, "gov50-final-shiny/clean_data/big_nodes.RDS")

big_edges <- big_network %>%
  left_join(big_nodes, by = c("source" = "label")) %>%
  rename(to = id) %>%
  left_join(big_nodes, by = c("network" = "label")) %>%
  rename(from = id) %>%
  rename(weight = count) %>%
  select(to, from, weight)

saveRDS(big_edges, "gov50-final-shiny/clean_data/big_edges.RDS")

big_network_graph <- graph_from_data_frame(d = big_edges, directed = F, vertices = big_nodes)

V(big_network_graph)$indegree <- degree(big_network_graph)

V(big_network_graph)$betweenness <- betweenness(big_network_graph, directed = FALSE, weights = E(big_network_graph)$weight)

ggraph(big_network_graph, layout = "lgl") +
  geom_edge_fan(aes(alpha = sqrt(weight))) +
  theme_void() +
  geom_node_text(aes(label = ifelse(
    V(big_network_graph)$in_corpus == TRUE,
    V(big_network_graph)$label,
    "")),
    color = "darkred",
    size = 2)



```



```{r, warning = FALSE}
#This section is about cleaning and graphing connections to non-telegram sources 

create_outside_connections <- function(data, name) {
  read_csv(data, col_types = cols(
  .default = col_character(),
  id = col_double(),
  messages__id = col_double(),
  messages__date = col_datetime(format = ""),
  messages__from_id = col_double(),
  messages__edited = col_datetime(format = ""),
  messages__width = col_double(),
  messages__height = col_double(),
  messages__reply_to_message_id = col_double(),
  messages__duration_seconds = col_double(),
  messages__actor_id = col_double(),
  messages__message_id = col_double(),
  messages__poll__closed = col_logical(),
  messages__poll__total_voters = col_double(),
  messages__poll__answers__voters = col_double(),
  messages__poll__answers__chosen = col_logical()
)) %>%
  select(messages__text__href) %>%
  rename(href = messages__text__href) %>%
  filter(!is.na(href)) %>%
  
  #\\ ensures that I am only grabbing the first instance of the pattern
  #I tried for a long time to just grab everything that fit within the pattern // (anything in between, up until the FIRST /
  #The closest I could get was the following, but it still didn't work
  #mutate(href_clean = sub("\\//.+/", "", href))
  #Instead I just found a package that works with urls
  
  mutate(href_parsed = map(href, ~url_parse(.))) %>%
  unnest(cols = c(href_parsed)) %>%
  select(domain) %>%
  mutate(source = name)
}

  nexta_live_outside_connections <- create_outside_connections("raw_data_csv/nexta_live.csv", "NEXTA Live")
  tutby_outside_connections <- create_outside_connections("raw_data_csv/tutby_official_raw.csv", "TUT.BY новости")
  nexta_tv_outside_connections <- create_outside_connections("raw_data_csv/raw_nexta_tv.csv", "NEXTA")
  belamova_outside_connections <- create_outside_connections("raw_data_csv/belamova.csv", "Беларусь головного мозга")
  mkbelarus_outside_connections <- create_outside_connections("raw_data_csv/mkbelarus.csv", "МАЯ КРАІНА БЕЛАРУСЬ")
  nashaniva_outside_connections <- create_outside_connections("raw_data_csv/nashaniva.csv", "Наша Нiва")
  onliner_outside_connections <- create_outside_connections("raw_data_csv/onliner.csv", "Onliner")
  euroradio_outside_connections <- create_outside_connections("raw_data_csv/euroradio.csv", "Euroradio")
  motolkohelp_outside_connections <- create_outside_connections("raw_data_csv/motolkohelp.csv", "#МотолькоПомоги этому городишко от 3% избавиться")
  kyky_outside_connections <- create_outside_connections("raw_data_csv/kyky.csv", "Кулуары KYKY")
  belsat_outside_connections <- create_outside_connections("raw_data_csv/belsat.csv", "Белсат")
  radiosvaboda_outside_connections <- create_outside_connections("raw_data_csv/radiosvaboda.csv", "Радыё Свабода — Беларусь")
  usylukashenko_outside_connections <- create_outside_connections("raw_data_csv/usylukashenko.csv", "Усы Лукашенко")
  tsikhanouskaya_outside_connections <- create_outside_connections("raw_data_csv/tsikhanouskaya.csv", "Светлана Тихановская")
  blackbookbelarus_outside_connections <- create_outside_connections("raw_data_csv/blackbookbelarus.csv", "Черная книга Беларуси")
  belteanews_outside_connections <- create_outside_connections("raw_data_csv/belteanews.csv", "Чай з малинавым варэннем")
  tribuna_by_outside_connections <- create_outside_connections("raw_data_csv/tribuna_by.csv", "Tribuna.com Беларусь")
  studenty_by_outside_connections <- create_outside_connections("raw_data_csv/studenty_by.csv", "отчислено")
  pressmvd_outside_connections <- create_outside_connections("raw_data_csv/pressmvd.csv", "Пресс-секретарь МВД Беларуси")
  mc_maxim_outside_connections <- create_outside_connections("raw_data_csv/mc_maxim.csv", "Работаем с Пушкиным!")
  viasna96_outside_connections <- create_outside_connections("raw_data_csv/viasna96.csv", "Вясна / Правы чалавека ў Беларусі")
  bajby_outside_connections <- create_outside_connections("raw_data_csv/bajby.csv", "БАЖ / BAJ")
  minsk_eastern_district_107_chat_outside_connections <- create_outside_connections("raw_data_csv/minsk_eastern_district_107_chat.csv", "Neighborhood Example 1")
  recall_deputy_lenchevskaya_outside_connections <- create_outside_connections("raw_data_csv/recall_deputy_lenchevskaya.csv", "Neighborhood Example 2")
  stop_gaiduk_outside_connections <- create_outside_connections("raw_data_csv/stop_gaiduk.csv", "Neighborhood Example 3")
  vybory_smotri_outside_connections <- create_outside_connections("raw_data_csv/vybory_smotri.csv", "Выборы видишь? | TUT.BY Политика")
  
  #Now I combine all of those into the whole network
  big_outside_network <- bind_rows(nexta_live_outside_connections, 
                           tutby_outside_connections,
                           nexta_tv_outside_connections,
                           belamova_outside_connections,
                           mkbelarus_outside_connections,
                           nashaniva_outside_connections,
                           onliner_outside_connections,
                           euroradio_outside_connections,
                           motolkohelp_outside_connections,
                           kyky_outside_connections,
                           belsat_outside_connections,
                           radiosvaboda_outside_connections,
                           usylukashenko_outside_connections,
                           tsikhanouskaya_outside_connections,
                           blackbookbelarus_outside_connections,
                           belteanews_outside_connections,
                           tribuna_by_outside_connections,
                           studenty_by_outside_connections,
                           pressmvd_outside_connections,
                           mc_maxim_outside_connections,
                           viasna96_outside_connections,
                           bajby_outside_connections,
                           minsk_eastern_district_107_chat_outside_connections,
                           recall_deputy_lenchevskaya_outside_connections,
                           stop_gaiduk_outside_connections,
                           vybory_smotri_outside_connections)
  
saveRDS(big_outside_network, "big_outside_network.RDS")

big_outside_network %>%
  group_by(domain) %>%
  summarize(total = n(), 
            .groups = "drop") %>%
  filter(domain != "t.me") %>%
  arrange(desc(total)) %>%
  slice(1:30) %>%
  ggplot(aes(x = total, y = fct_reorder(domain, total))) +
  geom_col() +
  labs(title = "Total external links",
       subtitle = "Mostly other social media networks, sites linked to telegram channels, and other news sites",
       x = "",
       y = "domain of link")
  
```


```{r text cleaning functions}
#This section is about cleaning the text data

clean_text <- function(data, name){
  read_csv(data, col_types = cols(
  .default = col_character(),
  id = col_double(),
  messages__id = col_double(),
  messages__date = col_datetime(format = ""),
  messages__from_id = col_double(),
  messages__edited = col_datetime(format = ""),
  messages__width = col_double(),
  messages__height = col_double(),
  messages__reply_to_message_id = col_double(),
  messages__duration_seconds = col_double(),
  messages__actor_id = col_double(),
  messages__message_id = col_double(),
  messages__poll__closed = col_logical(),
  messages__poll__total_voters = col_double(),
  messages__poll__answers__voters = col_double(),
  messages__poll__answers__chosen = col_logical()
))%>%
  select(messages__id, messages__date, messages__text, messages__text__text) %>%
  rename(id = messages__id, date = messages__date, text1 = messages__text, text2 = messages__text__text) %>%
  mutate(text_raw = ifelse(is.na(text1), text2, text1)) %>%
  select(id, date, text_raw) %>%
  filter(!is.na(text_raw)) %>%
  fill(id, date, .direction = "down") %>%
  
  #Removing line breaks proved to be difficult
  #I tried gsub("\n", "", text_raw) as well as:
  #[\r\n]
  #[\n]
  #the UTF-8 encoding U+000A with and without [] and <>. 
  #The ultimate answer was [\\n], credit to Wyatt Hurt for resolving this issue!
  
  mutate(text_no_n = gsub("[\\n]", "", text_raw)) %>%
  mutate(text_no_punc = removePunctuation(text_no_n)) %>%
  mutate(text_no_num = removeNumbers(text_no_punc)) %>%
  mutate(text_no_caps = lowerCase(text_no_num)) %>%
  
  #the following code creates a list column for which each word is a separate element
  #I initially tried to map, that did NOT work because it created a tibble which I could not unnest
  #Credit to Wyatt Hurt for resolving this issue!
  
  mutate(text_list = str_split(text_no_caps, " ")) %>%
  unnest(col = text_list) %>%
  rename(word_raw = text_list) %>%
  select(id, date, word_raw) %>%
  
  #Needed to remove stopwords before stemming because otherwise didn't catch them all
  
  filter(!word_raw %in% stopwords("ru")) %>%
  
  #This stemmer is very crude but it is the best R has. Python has much better options
  
  mutate(word_stemmed = stem_snowball(word_raw, "ru")) %>%
  mutate(word_stemmed = ifelse(
    word_stemmed == "",
    NA,
    word_stemmed
  )) %>%
  filter(!is.na(word_stemmed))
  
}

create_tokenized <- function(data, name) {
  clean_text(data, name) %>%
    group_by(id, date) %>%
    summarize(tokenized_words = list(word_stemmed), .groups = "drop") %>%
    mutate(source = name) 
}

count_words <- function(data){
  data %>%
  group_by(word_stemmed) %>%
  summarize(count = n(), .groups = "drop") %>%
  arrange(desc(count))
}

#Original code used to make functions 

raw_nexta <- read_csv("raw_data_csv/nexta_live.csv", col_types = cols(
  .default = col_character(),
  id = col_double(),
  messages__id = col_double(),
  messages__date = col_datetime(format = ""),
  messages__from_id = col_double(),
  messages__edited = col_datetime(format = ""),
  messages__width = col_double(),
  messages__height = col_double(),
  messages__reply_to_message_id = col_double(),
  messages__duration_seconds = col_double(),
  messages__actor_id = col_double(),
  messages__message_id = col_double(),
  messages__poll__closed = col_logical(),
  messages__poll__total_voters = col_double(),
  messages__poll__answers__voters = col_double(),
  messages__poll__answers__chosen = col_logical()
))

nexta_text_clean <- raw_nexta %>%
  select(messages__id, messages__date, messages__text, messages__text__text) %>%
  rename(id = messages__id, date = messages__date, text1 = messages__text, text2 = messages__text__text) %>%
  mutate(text_raw = ifelse(is.na(text1), text2, text1)) %>%
  select(id, date, text_raw) %>%
  filter(!is.na(text_raw)) %>%
  fill(id, date, .direction = "down") %>%
  
  #Removing line breaks proved to be difficult
  #I tried gsub("\n", "", text_raw) as well as:
  #[\r\n]
  #[\n]
  #the UTF-8 encoding U+000A with and without [] and <>. 
  #The ultimate answer was [\\n], credit to Wyatt Hurt for resolving this issue!
  
  mutate(text_no_n = gsub("[\\n]", "", text_raw)) %>%
  mutate(text_no_punc = removePunctuation(text_no_n)) %>%
  mutate(text_no_num = removeNumbers(text_no_punc)) %>%
  mutate(text_no_caps = lowerCase(text_no_num)) %>%
  
  #the following code creates a list column for which each word is a separate element
  #I initially tried to map, that did NOT work because it created a tibble which I could not unnest
  #Credit to Wyatt Hurt for resolving this issue!
  
  mutate(text_list = str_split(text_no_caps, " ")) %>%
  unnest(col = text_list) %>%
  rename(word_raw = text_list) %>%
  select(id, date, word_raw) %>%
  
  #Needed to remove stopwords before stemming because otherwise didn't catch them all
  
  filter(!word_raw %in% stopwords("ru")) %>%
  
  #This stemmer is very crude but it is the best R has. Python has much better options
  
  mutate(word_stemmed = stem_snowball(word_raw, "ru")) %>%
  
  #Here I am trying to remove words "" which might have been messing up the topic model
  
  mutate(word_stemmed = ifelse(
    word_stemmed == "",
    NA,
    word_stemmed
  )) %>%
  
  #Here I tried to remove emojis it did not work
  
 mutate(word_stemmed2 = str_replace(word_stemmed,
                                     pattern = !('[/^a-z\u0400-\u04FF]'), 
                                     replacement = "")) %>%
  filter(!is.na(word_stemmed))
  
view(nexta_text_clean)
#the following creates a tibble of texts (each text is a message) with a list of stemmed words in that text (this is the format needed for topic modelling)

nexta_texts <- nexta_text_clean %>%
  group_by(id, date) %>%
  summarize(tokenized_words = list(word_stemmed), .groups = "drop") %>%
  mutate(source = "NEXTA") 


#the following creates a list of the words in the text with the frequency (probably not going to be useful)

nexta_words <- nexta_text_clean %>%
  group_by(word_stemmed) %>%
  summarize(count = n(), .groups = "drop") %>%
  arrange(desc(count)) %>%
  mutate(source = "NEXTA")

#stm package (based on tm package)

```

```{r here I clean the actual functions}
nexta_live_tokenized <- create_tokenized("raw_data_csv/nexta_live.csv", "NEXTA Live")
  tutby_tokenized <- create_tokenized("raw_data_csv/tutby_official_raw.csv", "TUT.BY новости")
  nexta_tv_tokenized <- create_tokenized("raw_data_csv/raw_nexta_tv.csv", "NEXTA")
  belamova_tokenized <- create_tokenized("raw_data_csv/belamova.csv", "Беларусь головного мозга")
  mkbelarus_tokenized <- create_tokenized("raw_data_csv/mkbelarus.csv", "МАЯ КРАІНА БЕЛАРУСЬ")
  nashaniva_tokenized <- create_tokenized("raw_data_csv/nashaniva.csv", "Наша Нiва")
  onliner_tokenized <- create_tokenized("raw_data_csv/onliner.csv", "Onliner")
  euroradio_tokenized <- create_tokenized("raw_data_csv/euroradio.csv", "Euroradio")
  motolkohelp_tokenized <- create_tokenized("raw_data_csv/motolkohelp.csv", "#МотолькоПомоги этому городишко от 3% избавиться")
  kyky_tokenized <- create_tokenized("raw_data_csv/kyky.csv", "Кулуары KYKY")
  belsat_tokenized <- create_tokenized("raw_data_csv/belsat.csv", "Белсат")
  radiosvaboda_tokenized <- create_tokenized("raw_data_csv/radiosvaboda.csv", "Радыё Свабода — Беларусь")
  usylukashenko_tokenized <- create_tokenized("raw_data_csv/usylukashenko.csv", "Усы Лукашенко")
  tsikhanouskaya_tokenized <- create_tokenized("raw_data_csv/tsikhanouskaya.csv", "Светлана Тихановская")
  blackbookbelarus_tokenized <- create_tokenized("raw_data_csv/blackbookbelarus.csv", "Черная книга Беларуси")
  belteanews_tokenized <- create_tokenized("raw_data_csv/belteanews.csv", "Чай з малинавым варэннем")
  tribuna_by_tokenized <- create_tokenized("raw_data_csv/tribuna_by.csv", "Tribuna.com Беларусь")
  studenty_by_tokenized <- create_tokenized("raw_data_csv/studenty_by.csv", "отчислено")
  pressmvd_tokenized <- create_tokenized("raw_data_csv/pressmvd.csv", "Пресс-секретарь МВД Беларуси")
  mc_maxim_tokenized <- create_tokenized("raw_data_csv/mc_maxim.csv", "Работаем с Пушкиным!")
  viasna96_tokenized <- create_tokenized("raw_data_csv/viasna96.csv", "Вясна / Правы чалавека ў Беларусі")
  bajby_tokenized <- create_tokenized("raw_data_csv/bajby.csv", "БАЖ / BAJ")
  minsk_eastern_district_107_chat_tokenized <- create_tokenized("raw_data_csv/minsk_eastern_district_107_chat.csv", "Neighborhood Example 1")
  recall_deputy_lenchevskaya_tokenized <- create_tokenized("raw_data_csv/recall_deputy_lenchevskaya.csv", "Neighborhood Example 2")
  stop_gaiduk_tokenized <- create_tokenized("raw_data_csv/stop_gaiduk.csv", "Neighborhood Example 3")
  vybory_smotri_tokenized <- create_tokenized("raw_data_csv/vybory_smotri.csv", "Выборы видишь? | TUT.BY Политика")
  
  #Now I combine all of those into the whole corpus of texts
  tokenized_texts <- bind_rows(nexta_live_tokenized, 
                           tutby_tokenized,
                           nexta_tv_tokenized,
                           belamova_tokenized,
                           mkbelarus_tokenized,
                           nashaniva_tokenized,
                           onliner_tokenized,
                           euroradio_tokenized,
                           motolkohelp_tokenized,
                           kyky_tokenized,
                           belsat_tokenized,
                           radiosvaboda_tokenized,
                           usylukashenko_tokenized,
                           tsikhanouskaya_tokenized,
                           blackbookbelarus_tokenized,
                           belteanews_tokenized,
                           tribuna_by_tokenized,
                           studenty_by_tokenized,
                           pressmvd_tokenized,
                           mc_maxim_tokenized,
                           viasna96_tokenized,
                           bajby_tokenized,
                           minsk_eastern_district_107_chat_tokenized,
                           recall_deputy_lenchevskaya_tokenized,
                           stop_gaiduk_tokenized,
                           vybory_smotri_tokenized) %>%
    
    #Have to do below because keyATM needs a column with the words separated by spaces. str_c collapses the list.
    
    mutate(text = map_chr(tokenized_words, ~str_c(., collapse = " "))) %>%
    filter(text != "") %>%
    mutate(text = str_replace(text, pattern = "  ", replacement = " ")) %>%
    mutate(text = str_replace(text, pattern = "   ", replacement = " "))
  
saveRDS(tokenized_texts, "gov50-final-shiny/clean_data/tokenized_texts.RDS")

```

```{r topic modelling}
keywords = list(logistical = c("выход", "улиц", "площад", "район", "центр", "возл", "окол", "собира"),
                ideological = c("выбор", "лукашенк", "власт", "белорус", "свобод", "президент"))

keyATM_docs <- keyATM_read(tokenized_texts)

keyATM(docs = tokenized_texts,
       no_keyword_topics = 2,
       keywords = keywords,
       model = "covariates",
       model_settings = list(covariates_data = tokenized_texts %>%
                               select(source),
                             covariates_formula = ~ source))
```

```{r}
#some useful numbers about density of network

small_edges %>%
  group_by(from) %>%
  summarize(unique_connections = n(), .groups = "drop") %>%
  summarize(avg = mean(unique_connections))

small_edges %>%
  group_by(from) %>%
  summarize(avg_links_per_channel = mean(weight), .groups = "drop") %>%
  summarize(avg = mean(avg_links_per_channel))

outside_corpus_connections <- big_network %>%
  mutate(in_corpus = ifelse(network %in% small_nodes$label, TRUE, FALSE)) %>%
  filter(in_corpus == FALSE) %>%
  group_by(source) %>%
  summarize(unique_connections = n(), .groups = "drop")
view(outside_corpus_connections)

big_network %>%
  mutate(in_corpus = ifelse(network %in% small_nodes$label, TRUE, FALSE)) %>%
  filter(in_corpus == FALSE) %>%
  filter(source %in% c("Neighborhood Example 1", "Neighborhood Example 2", "Neighborhood Example 3")) %>%
  group_by(source) %>%
  summarize(unique_connections = n(), .groups = "drop") %>%
  summarize(avg = mean(unique_connections))

big_network %>%
  mutate(in_corpus = ifelse(network %in% small_nodes$label, TRUE, FALSE)) %>%
  filter(in_corpus == FALSE) %>%
  filter(!(source %in% c("Neighborhood Example 1", "Neighborhood Example 2", "Neighborhood Example 3"))) %>%
  group_by(source) %>%
  summarize(unique_connections = n(), .groups = "drop") %>%
  summarize(avg = mean(unique_connections))
```


